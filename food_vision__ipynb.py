# -*- coding: utf-8 -*-
"""FOOD_vision ._ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OQjJSz6kO5DfxW0N6odELfDpTs65Z3A-
"""

from google.colab import drive
drive.mount('/content/drive')

train_dir="/content/drive/MyDrive/10_food_classes_10_percent/10_food_classes_10_percent/train"
test_dir="/content/drive/MyDrive/10_food_classes_10_percent/10_food_classes_10_percent/test"

import tensorflow as tf

# Create a function to import an image and resize it to be able to be used with our model
def load_and_prep_image(filename, img_shape=224, scale=True):
  # Read in the image
  img = tf.io.read_file(filename)
  # Decode it into a tensor
  img = tf.image.decode_jpeg(img)
  # Resize the image
  img = tf.image.resize(img, [img_shape, img_shape])
  if scale:
    # Rescale the image (get all values between 0 and 1)
    return img/255.
  else:
    return img

import itertools
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix

import datetime

def create_tensorboard_callback(dir_name, experiment_name):
  """
  Creates a TensorBoard callback instand to store log files.

  Stores log files with the filepath:
    "dir_name/experiment_name/current_datetime/"

  Args:
    dir_name: target directory to store TensorBoard log files
    experiment_name: name of experiment directory (e.g. efficientnet_model_1)
  """
  log_dir = dir_name + "/" + experiment_name + "/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
  tensorboard_callback = tf.keras.callbacks.TensorBoard(
      log_dir=log_dir
  )
  print(f"Saving TensorBoard log files to: {log_dir}")
  return tensorboard_callback

# Plot the validation and training data separately
import matplotlib.pyplot as plt

def plot_loss_curves(history):
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  accuracy = history.history['accuracy']
  val_accuracy = history.history['val_accuracy']

  epochs = range(len(history.history['loss']))

  # Plot loss
  plt.plot(epochs, loss, label='training_loss')
  plt.plot(epochs, val_loss, label='val_loss')
  plt.title('Loss')
  plt.xlabel('Epochs')
  plt.legend()

  # Plot accuracy
  plt.figure()
  plt.plot(epochs, accuracy, label='training_accuracy')
  plt.plot(epochs, val_accuracy, label='val_accuracy')
  plt.title('Accuracy')
  plt.xlabel('Epochs')
  plt.legend();
# Create function to unzip a zipfile into current working directory
# (since we're going to be downloading and unzipping a few files)
import zipfile

def unzip_data(filename):
  zip_ref = zipfile.ZipFile(filename, "r")
  zip_ref.extractall()
  zip_ref.close()

import os

def walk_through_dir(dir_path):
  for dirpath, dirnames, filenames in os.walk(dir_path):
    print(f"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.")

from sklearn.metrics import accuracy_score, precision_recall_fscore_support

from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, compare_historys, walk_through_dir

walk_through_dir("/content/drive/MyDrive/10_food_classes_10_percent/10_food_classes_10_percent")

import tensorflow as tf
IMG_SIZE = (224, 224)
train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(train_dir,
                                                                                label_mode="categorical",
                                                                                image_size=IMG_SIZE)

test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,
                                                                label_mode="categorical",
                                                                image_size=IMG_SIZE,
                                                                shuffle=False)

checkpoint_path = "101_classes_10_percent_data_model_checkpoint"
checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,
                                                         save_weights_only=True, # save only the model weights
                                                         monitor="val_accuracy", # save the model weights which score the best validation accuracy
                                                         save_best_only=True) # only keep the best model weights on file (delete the rest)

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, models
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
data_augmentation = Sequential([
  layers.RandomFlip("horizontal"),
  layers.RandomRotation(0.2),
  layers.RandomZoom(0.2),
  layers.RandomHeight(0.2),
  layers.RandomWidth(0.2),
  layers.Rescaling(1./255),
], name ="data_augmentation")

"""##INCEPTION MODULE"""

base_model = tf.keras.applications.InceptionV3(include_top=False)
base_model.trainable = False

# Setup model architecture with trainable top layers
inputs = layers.Input(shape=(224, 224, 3), name="input_layer") # shape of input image
x = data_augmentation(inputs) # augment images (only happens during training)
x = base_model(x, training=False) # put the base model in inference mode so we can use it to extract features without updating the weights
x = layers.GlobalAveragePooling2D(name="global_average_pooling")(x) # pool the outputs of the base model
outputs = layers.Dense(len(train_data_10_percent.class_names), activation="softmax", name="output_layer")(x) # same number of outputs as classes
model_1 = tf.keras.Model(inputs, outputs)

model_1.summary()

model_1.compile(loss="categorical_crossentropy",
              optimizer=tf.keras.optimizers.Adam(), # use Adam with default settings
              metrics=["accuracy"])

history_all_classes_10_percent = model_1.fit(train_data_10_percent,
                                           epochs=5,
                                           validation_data=test_data,
                                           validation_steps=int(0.15 * len(test_data)), # evaluate on smaller portion of test data
                                           callbacks=[checkpoint_callback])

results_feature_extraction_model = model_1.evaluate(test_data)
results_feature_extraction_model

plot_loss_curves(history_all_classes_10_percent)

base_model.trainable = True

# Refreeze every layer except for the last 5
for layer in base_model.layers[:-5]:
  layer.trainable = False

model.compile(loss='categorical_crossentropy',
              optimizer=tf.keras.optimizers.Adam(1e-4), # 10x lower learning rate than default
              metrics=['accuracy'])

for layer in model.layers:
  print(layer.name, layer.trainable)

for layer_number, layer in enumerate(base_model.layers):
  print(layer_number, layer.name, layer.trainable)

fine_tune_epochs = 10 # model has already done 5 epochs, this is the total number of epochs we're after (5+5=10)

history_all_classes_10_percent_fine_tune = model.fit(train_data_10_percent,
                                                     epochs=fine_tune_epochs,
                                                     validation_data=test_data,
                                                     validation_steps=int(0.15 * len(test_data)), # validate on 15% of the test data
                                                     initial_epoch=history_all_classes_10_percent.epoch[-1])

results_all_classes_10_percent_fine_tune = model.evaluate(test_data)
results_all_classes_10_percent_fine_tune

pred_probs = model_1.predict(test_data, verbose=1)

len(pred_probs)

pred_probs.shape

pred_classes = pred_probs.argmax(axis=1)

pred_classes[:30]

y_labels = []
for images, labels in test_data.unbatch(): # unbatch the test data and get images and labels
  y_labels.append(labels.numpy().argmax()) # append the index which has the largest value (labels are one-hot)
y_labels[:10] # check what they look like (unshuffled)

len(y_labels)

from sklearn.metrics import accuracy_score
sklearn_accuracy = accuracy_score(y_labels, pred_classes)
sklearn_accuracy

from sklearn.metrics import classification_report
print(classification_report(y_labels, pred_classes))

"""##XCEPTION"""

base_model2 = tf.keras.applications.Xception(include_top=False)
base_model2.trainable = False

# Setup model architecture with trainable top layers
inputs = layers.Input(shape=(224, 224, 3), name="input_layer") # shape of input image
x = data_augmentation(inputs) # augment images (only happens during training)
x = base_model2(x, training=False) # put the base model in inference mode so we can use it to extract features without updating the weights
x = layers.GlobalAveragePooling2D(name="global_average_pooling")(x) # pool the outputs of the base model
outputs = layers.Dense(len(train_data_10_percent.class_names), activation="softmax", name="output_layer")(x) # same number of outputs as classes
model2 = tf.keras.Model(inputs, outputs)

model2.summary()

model2.compile(loss="categorical_crossentropy",
              optimizer=tf.keras.optimizers.Adam(), # use Adam with default settings
              metrics=["accuracy"])

history_all_classes_10_percent = model2.fit(train_data_10_percent,
                                           epochs=5,
                                           validation_data=test_data,
                                           validation_steps=int(0.15 * len(test_data)), # evaluate on smaller portion of test data
                                           callbacks=[checkpoint_callback])

results_feature_extraction_model2 = model2.evaluate(test_data)
results_feature_extraction_model2

pred_probs2 = model2.predict(test_data, verbose=1)

fine_tune_epochs = 10 # model has already done 5 epochs, this is the total number of epochs we're after (5+5=10)

history_all_classes_10_percent_fine_tune2 = model2.fit(train_data_10_percent,
                                                     epochs=fine_tune_epochs,
                                                     validation_data=test_data,
                                                     validation_steps=int(0.15 * len(test_data)), # validate on 15% of the test data
                                                     initial_epoch=history_all_classes_10_percent.epoch[-1])

results_all_classes_10_percent_fine_tune2 = model2.evaluate(test_data)
results_all_classes_10_percent_fine_tune2

pred_probs2 = model2.predict(test_data, verbose=1)

pred_classes2 = pred_probs2.argmax(axis=1)

sklearn_accuracy2 = accuracy_score(y_labels, pred_classes2)
sklearn_accuracy2

print(classification_report(y_labels, pred_classes2))

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

"""###xception module deep features for ml"""

def extract_deep_features(model_x, data_generator):
    features = []  # List to store the extracted features
    labels = []  # List to store the corresponding labels
    for inputs, labels_batch in data_generator:
        features_batch = model2.predict(inputs)  # Predicting the features using model
        features.extend(features_batch)
        labels.extend(labels_batch)
        if len(features) >= len(data_generator):
            break  # Break the loop if the number of extracted features exceeds the total number of samples
    return np.array(features), np.array(labels)  # Converting the list to numpy arrays

# Extract features from the pre-trained Xception model
train_features, train_labels = extract_deep_features(history_all_classes_10_percent_fine_tune2 , train_data_10_percent)
test_features, test_labels = extract_deep_features(history_all_classes_10_percent_fine_tune2, test_data)

# Flattening the extracted features
train_features_flat = train_features.reshape(train_features.shape[0], -1)
test_features_flat = test_features.reshape(test_features.shape[0], -1)

# Flattening one-hot encoded labels to 1-D array
train_labels_flat = np.argmax(train_labels, axis=1)
test_labels_flat = np.argmax(test_labels, axis=1)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(train_features_flat, train_labels_flat, test_size=0.2, random_state=42)

# Training the ML models
models = {
    "Logistic Regression": LogisticRegression(),
    "Support Vector Machine": SVC(),
    "Random Forest": RandomForestClassifier(),
    "Decision Trees": DecisionTreeClassifier()
}

# Iterating over each model and Evaluating its performance
for name, model in models.items():
    model.fit(X_train, y_train)  # Training the model
    y_pred = model.predict(X_test)  # Predicting labels for the test set

    # Calculating the various performance metrices
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    cm = confusion_matrix(y_test, y_pred)

    # Printing the performance metrics for the current model
    print(f"Model: {name}")
    print(f"Accuracy: {accuracy}")
    print(f"Precision: {precision}")
    print(f"Recall: {recall}")
    print(f"F1-score: {f1}")
    print(f"Confusion Matrix:\n{cm}\n")

"""##deep features for inception"""

def extract_deep_features(model_x, data_generator):
    features = []  # List to store the extracted features
    labels = []  # List to store the corresponding labels
    for inputs, labels_batch in data_generator:
        features_batch = model_1.predict(inputs)  # Predicting the features using model
        features.extend(features_batch)
        labels.extend(labels_batch)
        if len(features) >= len(data_generator):
            break  # Break the loop if the number of extracted features exceeds the total number of samples
    return np.array(features), np.array(labels)  # Converting the list to numpy arrays

train_features, train_labels = extract_deep_features(history_all_classes_10_percent  , train_data_10_percent)
test_features, test_labels = extract_deep_features(history_all_classes_10_percent, test_data)

# Flattening the extracted features
train_features_flat = train_features.reshape(train_features.shape[0], -1)
test_features_flat = test_features.reshape(test_features.shape[0], -1)

# Flattening one-hot encoded labels to 1-D array
train_labels_flat = np.argmax(train_labels, axis=1)
test_labels_flat = np.argmax(test_labels, axis=1)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(train_features_flat, train_labels_flat, test_size=0.2, random_state=42)

# Training the ML models
models = {
    "Logistic Regression": LogisticRegression(),
    "Support Vector Machine": SVC(),
    "Random Forest": RandomForestClassifier(),
    "Decision Trees": DecisionTreeClassifier()
}

# Iterating over each model and Evaluating its performance
for name, model in models.items():
    model.fit(X_train, y_train)  # Training the model
    y_pred = model.predict(X_test)  # Predicting labels for the test set

    # Calculating the various performance metrices
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    cm = confusion_matrix(y_test, y_pred)

    # Printing the performance metrics for the current model
    print(f"Model: {name}")
    print(f"Accuracy: {accuracy}")
    print(f"Precision: {precision}")
    print(f"Recall: {recall}")
    print(f"F1-score: {f1}")
    print(f"Confusion Matrix:\n{cm}\n")

"""#####inference:
Both inception and xception models performed well.
*Especially inception perfmored well in terms of accuracy. that too in SVM classifier.

2.The dataset itself is a small dataset , consosting of 70 train images per class and 250 test images per class. Totally 10 classes .

3.Overall metrics of all models ML were good. but SVM outperformed other models in Inception model,

4.In terms of Xception , Logistic regresssion performed better than other ML models.



5.Changes i made from last try, I didnt use best_model.h5 . Here i used a normal inception and xception models. In inception too i used some layers (trained last few layers) . Also use of validation steps to make fewer steps at a time to compensate for very low train(70) for test(250) images.

6. Even tried adding more dense layers and epochs , but that gave somewhat better results than h5 best model.. But the present model perfroms best than all


"""

#inference:
#Both inception and xception models performed well. *Especially inception perfmored well in terms of accuracy. that too in SVM classifier.

#2.The dataset itself is a small dataset , consosting of 70 train images per class and 250 test images per class. Totally 10 classes .

#3.Overall metrics of all models ML were good. but SVM outperformed other models in Inception model,

#4.In terms of Xception , Logistic regresssion performed better than other ML models.

#5.Changes i made from last try, I didnt use best_model.h5 . Here i used a normal inception and xception models. In inception too i used some layers (trained last few layers) . Also use of validation steps to make fewer steps at a time to compensate for very low train(70) for test(250) images.

#Even tried adding more dense layers and epochs , but that gave somewhat better results than h5 best model.. But the present model perfroms best than all